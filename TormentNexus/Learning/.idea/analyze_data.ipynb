{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The First Step is to Activate the Kernal or conda environment within this file\n",
    "# In VS Code, click \"Select Kernal\" in the top right\n",
    "# Select Python Environments then activate the Task2 Kernal (you should now see \"Task2 (Python 3.10.15)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:59:02.491810Z",
     "start_time": "2024-10-17T18:59:02.482900Z"
    }
   },
   "source": [
    "# Next you must make sure you can run Jupyter Notebooks\n",
    "# Run this cell, you may be prompted to install necessary software. Please do so.\n",
    "\n",
    "print(\"hello world\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:59:23.500838Z",
     "start_time": "2024-10-17T18:59:04.690807Z"
    }
   },
   "source": [
    "# Import Modules\n",
    "# You previously install libraries into your conda environment. You must now import those libraries into this script.\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import spikeinterface as si\n",
    "from spikeinterface import widgets, exporters, postprocessing, qualitymetrics, sorters\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T00:35:45.380046Z",
     "start_time": "2024-10-20T00:35:45.376896Z"
    }
   },
   "source": [
    "# Get Directory information\n",
    "# Next you must establish the directories used. This will allow python to accesss the recording and sorting data.\n",
    "\n",
    "current_directory = \"\\\\Users\\\\ivank\\\\OneDrive\\\\Desktop\\\\Task 2\" # different for each device\n",
    "output_folder = \"utah_organoids_output\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T00:35:47.073075Z",
     "start_time": "2024-10-20T00:35:47.069082Z"
    }
   },
   "source": [
    "# Understanding how to navigate the output folder\n",
    "# This folder is organized by organoid_id/drug_name/paramset_idx/sorter_name/\n",
    "# In the scope of this Task, some of these organization folders are unesscesary\n",
    "# I also understand we aren't dealing with organoids. But this is how our sessions are organized so please use this format.\n",
    "\n",
    "organoid_id = \"MB01\" # Mouse Brain 1\n",
    "drug_name = \"Control\" # All sessions are controlled in this data (no drugs used)\n",
    "sorter_name = \"spykingcircus2\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T00:35:48.754466Z",
     "start_time": "2024-10-20T00:35:48.749248Z"
    }
   },
   "source": [
    "# Create output directories to access the data\n",
    "\n",
    "def get_output_dir(organoid_id, drug_name, sorter_name):\n",
    "\n",
    "    sorter_to_paramset = { \n",
    "        \"spykingcircus2\":30,\n",
    "        \"tridesclous2\":26\n",
    "    } # Each paramset index contains the default parameters for each of the spike sorters. 26 and 30 are arbitrary numbers to call upon those parameters.\n",
    "    paramset_idx = sorter_to_paramset[sorter_name]    \n",
    "\n",
    "    output_dir = \"/\".join([current_directory, output_folder, organoid_id, drug_name, str(paramset_idx), sorter_name])\n",
    "    return output_dir\n",
    "\n",
    "print(get_output_dir(organoid_id, drug_name, sorter_name))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Users\\ivank\\OneDrive\\Desktop\\Task 2/utah_organoids_output/MB01/Control/30/spykingcircus2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T00:37:40.715472Z",
     "start_time": "2024-10-20T00:37:40.710777Z"
    }
   },
   "source": [
    "# The data avaiable for you to access is in npy and csv files. Functions will be avaiable to fetch this data given a directory. \n",
    "# Combine the output dir with one listed below to access these files.\n",
    "\n",
    "# Different sorters store different information in different formats. Here are dictionaries with dirs for all accesible information\n",
    "\n",
    "# spykingcircus2\n",
    "spykingcircus_dirs = {\n",
    "    \"spikes_dir\": \"spike_sorting/sorter_output/sorting\", # spikes.npy\n",
    "    \"peaks_dir\": \"spike_sorting/sorter_output/motion\", # peaks.npy , peak_locations.npy\n",
    "    \"motion_dir\": \"spike_sorting/sorter_output/motion/motion\", # displacement_seg0.npy , spatial_bins_um.npy , temporal_bins_s_seg0.npy\n",
    "    \"sparsity_dir\": \"sorting_analyzer\", # sparsity_mask.npy\n",
    "}\n",
    "tridesclous_dirs = {\n",
    "    \"spikes_dir\": \"spike_sorting/sorter_output/sorting\", # spikes.npy\n",
    "    \"peaks_dir\": \"spike_sorting/sorter_output\", # all_peaks.npy, clustering_label.npy, noise_levels.npy, peaks.npy, spikes.npy\n",
    "    \"sparsity_dir\": \"spike_sorting/sorter_output/features\", # peaks.npy, sparse_mask.npy, sparse_tsvd.npy, sparse_wfs.npy\n",
    "    \"pre-peeler_dir\": \"spike_sorting/sorter_output/sorting_pre_peeler\" # spikes.npy\n",
    "}\n",
    "\n",
    "# We also ran a sorting analyzer (done by spikeinterface) to extract important information. This format is global across sorters.\n",
    "def extensions_dir(extension: str): # both sorters\n",
    "\n",
    "    extensions = [\n",
    "        \"amplitude_scalings\", # amplitude_scalings.npy , collision_mask.npy\n",
    "        \"correlograms\", # bins.npy , ccgs.npy\n",
    "        \"isi_histograms\", # bins.npy , isi_histograms.npy\n",
    "        \"noise_levels\", # noise_levels.npy\n",
    "        \"principal_components\", # pca_projection.npy\n",
    "        \"quality_metrics\", # metrics.csv\n",
    "        \"random_spikes\", # random_spikes_indices.npy\n",
    "        \"spike_amplitudes\", # amplitudes.npy\n",
    "        \"spike_locations\", # spike_locations.npy\n",
    "        \"template_metrics\", # metrics.csv\n",
    "        \"template_similarity\", # simlarity.npy\n",
    "        \"templates\", # average.npy , std.npy\n",
    "        \"unit_locations\", # unit_locations.npy\n",
    "        \"waveforms\", # waveforms.npy\n",
    "    ]\n",
    "\n",
    "    if extension in extensions:\n",
    "        return f\"sorting_analyzer/extensions/{extension}\"\n",
    "    \n",
    "    else:\n",
    "        raise(\"Enter a Valid Extension\")\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:59:47.743885Z",
     "start_time": "2024-10-17T18:59:47.721831Z"
    }
   },
   "source": [
    "# Fetch data functions\n",
    "# Use these functions to access the needed data:\n",
    "# You need to input a datapath (made from the options above)\n",
    "# The function will output the files contents. In the event there are two files within the directory, the function will output a dictionary of both\n",
    "# files. If you specify a specific file it will output that files contents.\n",
    "\n",
    "def fetch_npy(path_to_folder: str , file=None):\n",
    "\n",
    "    file_names = os.listdir(path_to_folder)\n",
    "    npy_files = [file for file in file_names if file.endswith('.npy')]\n",
    "\n",
    "    if len(npy_files) == 1: \n",
    "        # If only one numpy file, return the numpy array\n",
    "        return np.load(path_to_folder + \"/\" + npy_files[0])\n",
    "    \n",
    "    elif len(npy_files) > 1:\n",
    "\n",
    "        if file is None:\n",
    "            \n",
    "            numpy_dict = {}\n",
    "            for npy_file in npy_files:\n",
    "                numpy_dict[npy_file] = np.load(path_to_folder + \"/\" + npy_file)\n",
    "            return numpy_dict    \n",
    "        \n",
    "        else:\n",
    "            return np.load(path_to_folder + \"/\" + file)\n",
    "        \n",
    "    else:\n",
    "        raise(\"No npy files in the given directory\")\n",
    "\n",
    "\n",
    "def fetch_csv(path_to_folder: str , file=None):\n",
    "\n",
    "    file_names = os.listdir(path_to_folder)\n",
    "    csv_files = [file for file in file_names if file.endswith('.csv')]\n",
    "\n",
    "    if len(csv_files) == 1: \n",
    "        # If only one numpy file, return the numpy array\n",
    "        return np.load(path_to_folder + \"/\" + csv_files[0])\n",
    "    \n",
    "    elif len(csv_files) > 1:\n",
    "\n",
    "        if file is None:\n",
    "            \n",
    "            csv_dict = {}\n",
    "            for csv_file in csv_files:\n",
    "                csv_dict[csv_file] = pd.read_csv(path_to_folder + \"/\" + csv_file)\n",
    "            return csv_dict    \n",
    "        \n",
    "        else:\n",
    "            return pd.read_csv(path_to_folder + \"/\" + file)\n",
    "        \n",
    "    else:\n",
    "        raise(\"No csv files in the given directory\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must now analyze the data provided. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
